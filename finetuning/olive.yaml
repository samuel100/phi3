# yaml-language-server: $schema=https://microsoft.github.io/Olive/schema.json
input_model:
  type: PyTorchModel
  config:
    hf_config:
      model_name: microsoft/Phi-3-mini-4k-instruct
      task: text-generation
      from_pretrained_args:
        trust_remote_code: true  

data_configs:
  - name: dataset_train
    type: HuggingfaceContainer
    load_dataset_config:
      params:
        data_name: json
        data_files: data/dataset-classification.json
        split: train
    pre_process_data_config:
      params:
        dataset_type: corpus
        text_cols:
          - phrase
          - tone
        text_template: <|user|>\n{phrase}<|end|>\n<|assistant|>\n{tone}<|end|>
        corpus_strategy: join
        source_max_len: 1024
        pad_to_max_len: false
        use_attention_mask: false

passes:
  lora:
    type: LoRA
    config:
      target_modules:
        - o_proj
        - qkv_proj
      lora_r: 64
      lora_alpha: 64
      lora_dropout: 0.1
      train_data_config: dataset_train
      eval_dataset_size: 0.3
      training_args:
        seed: 0
        data_seed: 42
        per_device_train_batch_size: 1
        per_device_eval_batch_size: 1
        gradient_accumulation_steps: 4
        gradient_checkpointing: false
        learning_rate: 0.0001
        max_steps: 20
        evaluation_strategy: steps
        adam_beta2: 0.999
        max_grad_norm: 0.3
        report_to: wandb
  merge_weights:
    type: MergeAdapterWeights
  model_builder:
    type: ModelBuilder
    config:
      precision: int4

engine:
  target:
    type: LocalSystem
    config:
      accelerators:
        - device: gpu
          execution_providers:
            - CUDAExecutionProvider
  search_strategy: false
  cache_dir: .olive-cache
  output_dir: models/lora
  packaging_config:
    type: AzureMLModels
    name: lora-model
    config:
      version: 1
      description: A lora fine-tuned phi3 model to handle phrase classification

azureml_client:
  aml_config_path: ./aml-config.json




      